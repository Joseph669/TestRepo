{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "test.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Joseph669/TestRepo/blob/master/test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "DSo2hbAQ-_Gx",
        "colab_type": "code",
        "outputId": "264425da-cd6d-4e8e-82e6-b55f4f4be4b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        }
      },
      "cell_type": "code",
      "source": [
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}\n",
        "\n",
        "# 挂载Google Drive\n",
        "!mkdir -p drive\n",
        "!google-drive-ocamlfuse drive\n",
        "\n",
        "import os\n",
        "os.chdir(\"/content/drive/HRV\")\n",
        "!ls"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "E: Package 'python-software-properties' has no installation candidate\n",
            "Selecting previously unselected package google-drive-ocamlfuse.\n",
            "(Reading database ... 131304 files and directories currently installed.)\n",
            "Preparing to unpack .../google-drive-ocamlfuse_0.7.3-0ubuntu1~ubuntu18.04.1_amd64.deb ...\n",
            "Unpacking google-drive-ocamlfuse (0.7.3-0ubuntu1~ubuntu18.04.1) ...\n",
            "Setting up google-drive-ocamlfuse (0.7.3-0ubuntu1~ubuntu18.04.1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "··········\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "Please enter the verification code: Access token retrieved correctly.\n",
            "Data\t\tfrequencyDomain.py    panTompkins.py  relax.csv\n",
            "DFA.py\t\tget_feats_csv.py      poincare.py     stress.csv\n",
            "feats_data.csv\tmultiScaleEntropy.py  __pycache__     timeDomain.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Ri-TX-uy1AP7",
        "colab_type": "code",
        "outputId": "b2a1eac9-d183-47eb-8221-56402790b68c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66
        }
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir(\"/content/drive/HRV\")\n",
        "!ls"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data\t\tfrequencyDomain.py    panTompkins.py  relax.csv\n",
            "DFA.py\t\tget_feats_csv.py      poincare.py     stress.csv\n",
            "feats_data.csv\tmultiScaleEntropy.py  __pycache__     timeDomain.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "tPi8D58j2oct",
        "colab_type": "code",
        "outputId": "974295cd-21bc-4edb-d9f1-c4050e35a96a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        }
      },
      "cell_type": "code",
      "source": [
        "from get_feats_csv import get_feats_file\n",
        "\n",
        "get_feats_file()\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-0f567319a2d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mget_feats_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'get_feats_file' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "EMUljjYocNka",
        "colab_type": "code",
        "outputId": "bceaaca7-c87b-40f8-f01c-d1dc2eedbae2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 150
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.svm import SVC\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection  import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib as plt \n",
        "import pandas as pd\n",
        "from random import sample\n",
        "\n",
        "file_name = 'feats_data.csv'\n",
        "#file_name = 'test_feats_data.csv'\n",
        "# feats_tuple     = sample(range(0,10), 3)\n",
        "feats_tuple     = (0,1,7,13,14)\n",
        "\n",
        "file_data = np.array(pd.read_csv(file_name))\n",
        "feats     = file_data[:, feats_tuple]\n",
        "#feats     = preprocessing.StandardScaler().fit_transform(feats)\n",
        "label     = file_data[:, -1]\n",
        "x_train,x_test,y_train,y_test = train_test_split(feats, label, test_size=0.3, random_state=2)\n",
        "clf = SVC(C=900, coef0=0, kernel='rbf', gamma='auto').fit(x_train, y_train) #调参\n",
        "\n",
        "\n",
        "print('----------------------------------------------------------')        \n",
        "print('----------------------------------------------------------')\n",
        "print('train accuracy:' + str(clf.score(x_train, y_train)))\n",
        "print(confusion_matrix(y_train, clf.predict(x_train)))\n",
        "print('test accuracy:' + str(clf.score(x_test, y_test)))\n",
        "print(confusion_matrix(y_test, clf.predict(x_test)))\n"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------\n",
            "----------------------------------------------------------\n",
            "train accuracy:0.7142857142857143\n",
            "[[20  2]\n",
            " [10 10]]\n",
            "test accuracy:0.5555555555555556\n",
            "[[7 1]\n",
            " [7 3]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "VNBe9oUKCBsT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        },
        "outputId": "d364a88e-7083-4f7e-c590-c3b9f822c89d"
      },
      "cell_type": "code",
      "source": [
        "from panTompkins import panTompkins\n",
        "from timeDomain import timeDomain\n",
        "from frequencyDomain import frequencyDomain\n",
        "import gc, os, platform, csv\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from poincare import eclipseFittingMethod, hraMethod, correlationCoef\n",
        "\n",
        "output_file = 'test_feats_data.csv'\n",
        "\n",
        "# video2 features data as test set\n",
        "# 3min relax + 2min stress + 2min relax + 3min stress\n",
        "def get_test_feats():\n",
        "    if os.path.exists(output_file):\n",
        "        os.remove(output_file)  \n",
        "    for num in range(4, 19):\n",
        "        print('-----------------------'+str(num)+'---------------------------')\n",
        "        abs_path = os.getcwd() + '/Data/NO'  + str(num) + '/' + 'video2.csv'\n",
        "        if (platform.system() == 'Windows'):\n",
        "            abs_path.replace('/', '\\\\')\n",
        "        all_data = (np.array(pd.read_csv(abs_path)))[:,1]\n",
        "        get_single_tester_feats(all_data[     0:  54000], 'relax')\n",
        "        get_single_tester_feats(all_data[ 54000:  90000], 'stress')\n",
        "        get_single_tester_feats(all_data[ 90000: 126000], 'relax')\n",
        "        get_single_tester_feats(all_data[126000:       ], 'stress')\n",
        "        del all_data\n",
        "        gc.collect()\n",
        "\n",
        "def get_single_tester_feats(csv_data, file_type):\n",
        "    label = 1 if file_type=='stress' else 0\n",
        "    R_peak_locs = panTompkins(ECG=csv_data, fs=300, plot=0)\n",
        "    RR_interval_series = (np.diff(R_peak_locs)) / 300\n",
        "    timeDomainFeats = timeDomain(RR_interval_series)\n",
        "    freqDomainFeats = frequencyDomain(RR_interval_series)\n",
        "    fittingFeats = eclipseFittingMethod(RR_interval_series)\n",
        "    hraFeats = hraMethod(RR_interval_series)\n",
        "    feats_list = [val for val in timeDomainFeats.values()] + \\\n",
        "        [val for val in freqDomainFeats.values()] + \\\n",
        "        [val for val in fittingFeats.values()] + \\\n",
        "        [val for val in hraFeats.values()] + \\\n",
        "        [correlationCoef(RR_interval_series), label]\n",
        "    \n",
        "    if os.path.exists(output_file):\n",
        "        out = open(output_file, 'a', newline='')\n",
        "        csv_write = csv.writer(out,dialect='excel')\n",
        "        csv_write.writerow(feats_list)       # write feats\n",
        "    else:    # 不存在文件则写入header+data\n",
        "        feats_header_list = [key for key in timeDomainFeats.keys()] + \\\n",
        "            [key for key in freqDomainFeats.keys()] + \\\n",
        "            [key for key in fittingFeats.keys()] + \\\n",
        "            [key for key in hraFeats.keys()] + \\\n",
        "            ['CorrCoef','label']\n",
        "        out = open(output_file, 'a', newline='')\n",
        "        csv_write = csv.writer(out,dialect='excel')\n",
        "        csv_write.writerow(feats_header_list) # write header \n",
        "        csv_write.writerow(feats_list)        # write feats\n",
        "\n",
        "\n",
        "get_test_feats()"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-----------------------4---------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:156: DeprecationWarning: elementwise != comparison failed; this will raise an error in the future.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "-----------------------5---------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:156: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "-----------------------6---------------------------\n",
            "-----------------------7---------------------------\n",
            "-----------------------8---------------------------\n",
            "-----------------------9---------------------------\n",
            "-----------------------10---------------------------\n",
            "-----------------------11---------------------------\n",
            "-----------------------12---------------------------\n",
            "-----------------------13---------------------------\n",
            "-----------------------14---------------------------\n",
            "-----------------------15---------------------------\n",
            "-----------------------16---------------------------\n",
            "-----------------------17---------------------------\n",
            "-----------------------18---------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "8DlDJdowzwEO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from scipy.io import loadmat\n",
        "import numpy as np\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection  import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn import preprocessing\n",
        "import matplotlib as plt \n",
        "import pandas as pd\n",
        "from random import sample\n",
        "\n",
        "# feats_tuple     = sample(range(0,10), 3)\n",
        "# feats_tuple  = (1,4,6,7,17)\n",
        "\n",
        "def show_result(feats):\n",
        "    relax_file      = 'relax.csv'\n",
        "    stress_file     = 'stress.csv'\n",
        "    relax_feats     = (np.array(pd.read_csv(relax_file)))[:,feats_tuple]\n",
        "    stress_feats    = (np.array(pd.read_csv(stress_file)))[:,feats_tuple]\n",
        "    relax_feats  = preprocessing.MinMaxScaler().fit_transform(relax_feats)\n",
        "    stress_feats = preprocessing.MinMaxScaler().fit_transform(stress_feats)\n",
        "    relax_train     =   relax_feats[0:25]\n",
        "    stress_train    =   stress_feats[0:25]\n",
        "    relax_test      =   relax_feats[25:30]\n",
        "    stress_test     =   stress_feats[25:30]\n",
        "\n",
        "    # print(relax_test.shape)\n",
        "    X = np.concatenate((relax_train, stress_train), axis=0)\n",
        "    Y = np.concatenate((np.zeros((25,1)), np.ones((25,1)))) #relax=0, stress=1\n",
        "    # print(np.shape(relax_test[0:1, :])) # (1, 5)\n",
        "    clf = SVC(C=pow(10, 4), coef0=0, kernel='rbf', gamma='auto').fit(X, Y) #调参\n",
        "\n",
        "    Y_h = clf.predict(X) \n",
        "    total = 0\n",
        "    for i, ele in enumerate(Y_h):\n",
        "        if ele == Y[i]:\n",
        "            total += 1\n",
        "    print('----------------------------------------------------------')        \n",
        "    print('----------------------------------------------------------')\n",
        "    print(feats)\n",
        "    print(confusion_matrix(Y, Y_h))\n",
        "    print('accuracy:%s%%' %str(total/np.size(Y)*100))\n",
        "    print('relax_test:' + str(clf.predict(relax_test)))\n",
        "    print(clf.score(relax_test, np.zeros((5, 1))))\n",
        "    print('stress_test:'+ str(clf.predict(stress_test)))\n",
        "    print(clf.score(stress_test, np.ones((5, 1))))\n",
        "\n",
        "    \n",
        "for i in range(0, 10):    \n",
        "    feats_tuple = sample(range(0,17), 4)\n",
        "    show_result(feats_tuple)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GZJlsDzAq1sv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import csv,os\n",
        "from sklearn import preprocessing\n",
        "\n",
        "\n",
        "output_file = 'subtraction.csv'\n",
        "if os.path.exists(output_file):\n",
        "    os.remove(output_file)\n",
        "    \n",
        "relax = (np.array(pd.read_csv('relax.csv')))[:, :]\n",
        "stress = (np.array(pd.read_csv('stress.csv')))[:, :]\n",
        "\n",
        "subtraction = stress - relax\n",
        "#subtraction = preprocessing.MinMaxScaler().fit_transform(subtraction)\n",
        "for row in subtraction:\n",
        "    out = open(output_file, 'a', newline='')\n",
        "    csv_write = csv.writer(out,dialect='excel')\n",
        "    csv_write.writerow(row)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-5ygJjCb0Uip",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from scipy.io import loadmat\n",
        "import numpy as np\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection  import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib as plt \n",
        "import pandas as pd\n",
        "from random import sample\n",
        "\n",
        "feats_tuple     = sample(range(0,10), 3)\n",
        "relax_file      = 'relax.csv'\n",
        "stress_file     = 'stress.csv'\n",
        "relax_feats     = (np.array(pd.read_csv(relax_file)))[:,feats_tuple]\n",
        "stress_feats    = (np.array(pd.read_csv(stress_file)))[:,feats_tuple]\n",
        "\n",
        "relax_train     =   relax_feats[0:25]\n",
        "stress_train    =   stress_feats[0:25]\n",
        "relax_test      =   relax_feats[25:30]\n",
        "stress_test     =   stress_feats[25:30]\n",
        "\n",
        "# print(relax_test.shape)\n",
        "X = np.concatenate((relax_train, stress_train), axis=0)\n",
        "Y = np.concatenate((np.zeros((25,1)), np.ones((25,1)))) #relax=0, stress=1\n",
        "# print(np.shape(relax_test[0:1, :])) # (1, 5)\n",
        "clf = SVC(C=pow(10, 4), coef0=0, kernel='rbf', gamma='auto').fit(X, Y) #调参\n",
        "\n",
        "Y_h = clf.predict(X) \n",
        "total = 0\n",
        "for i, ele in enumerate(Y_h):\n",
        "    if ele == Y[i]:\n",
        "        total += 1\n",
        "print('----------------------------------------------------------')        \n",
        "print('----------------------------------------------------------')\n",
        "print(confusion_matrix(Y, Y_h))\n",
        "print('accuracy:%s%%' %str(total/np.size(Y)*100))\n",
        "print('relax_test:' + str(clf.predict(relax_test)))\n",
        "print(clf.score(relax_test, np.zeros((5, 1))))\n",
        "print('stress_test:'+ str(clf.predict(stress_test)))\n",
        "print(clf.score(stress_test, np.ones((5, 1))))\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ERvFKb1kYvxq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from hrvanalysis import remove_outliers, remove_ectopic_beats, interpolate_nan_values, get_time_domain_features, get_geometrical_features,get_frequency_domain_features, get_csi_cvi_features, get_poincare_plot_features, get_sampen\n",
        "\n",
        "# rr_intervals_list contains integer values of RR-interval\n",
        "abs_path = os.getcwd() + '/Data/NO13/video1.csv'\n",
        "csv_data = pd.read_csv(abs_path)\n",
        "all_data = (np.array(csv_data))[:,1]\n",
        "\n",
        "data2 = all_data[90000: ]\n",
        "R_peak_locs = panTompkins(ECG=data2, fs=300, plot=0)\n",
        "R_peak_shift= [0] + R_peak_locs[0:-1]\n",
        "RR_interval_series = ((np.array(R_peak_locs) - np.array(R_peak_shift))[1:]) / 300 *1000\n",
        "rr_intervals_without_outliers = remove_outliers(rr_intervals=rr_intervals_list,  \n",
        "                                                low_rri=300, high_rri=2000)\n",
        "# This replace outliers nan values with linear interpolation\n",
        "interpolated_rr_intervals = interpolate_nan_values(rr_intervals=rr_intervals_without_outliers,\n",
        "                                                   interpolation_method=\"linear\")\n",
        "\n",
        "# This remove ectopic beats from signal\n",
        "nn_intervals_list = remove_ectopic_beats(rr_intervals=interpolated_rr_intervals, method=\"malik\")\n",
        "# This replace ectopic beats nan values with linear interpolation\n",
        "interpolated_nn_intervals = interpolate_nan_values(rr_intervals=nn_intervals_list)\n",
        "\n",
        "time_domain_features = get_time_domain_features(nn_intervals_list)\n",
        "frequency_domain_features = get_frequency_domain_features(nn_intervals_list) \n",
        "geometrical_features = get_geometrical_features(nn_intervals_list)\n",
        "poincare_plot_features = get_poincare_plot_features(nn_intervals_list)\n",
        "csi_cvi_features = get_csi_cvi_features(nn_intervals_list)\n",
        "for key in (time_domain_features.keys()):\n",
        "    print(key+':\\t'+str(time_domain_features[key]))\n",
        "for key in (frequency_domain_features.keys()):\n",
        "    print(key+':\\t'+str(frequency_domain_features[key]))\n",
        "for key in (geometrical_features.keys()):\n",
        "    print(key+':\\t'+str(geometrical_features[key]))\n",
        "for key in (poincare_plot_features.keys()):\n",
        "    print(key+':\\t'+str(poincare_plot_features[key]))\n",
        "for key in (csi_cvi_features.keys()):\n",
        "    print(key+':\\t'+str(csi_cvi_features[key]))    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7WPoqDsnGC_T",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from panTompkins import panTompkins\n",
        "from timeDomain import timeDomain\n",
        "from frequencyDomain import frequencyDomain\n",
        "import gc, os, platform, csv\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from poincare import eclipseFittingMethod, hraMethod, correlationCoef\n",
        "\n",
        "output_file = 'feats_data.csv'\n",
        "data_len = 90000\n",
        "\n",
        "def get_feats_file():\n",
        "    if os.path.exists(output_file):\n",
        "        os.remove(output_file)\n",
        "\n",
        "    get_all_tester_feats('relax')     \n",
        "    get_all_tester_feats('stress')        \n",
        "\n",
        "\n",
        "def get_all_tester_feats(file_type):\n",
        "    assert file_type =='relax' or file_type == 'stress'\n",
        "\n",
        "    if file_type == 'relax':\n",
        "        input_file = 'video1.csv'\n",
        "    elif file_type == 'stress':\n",
        "        input_file = 'video3.csv'\n",
        "\n",
        "    for num in range(4, 19):\n",
        "        print('-----------------------'+str(num)+'---------------------------')\n",
        "        abs_path = os.getcwd() + '/Data/NO'  + str(num) + '/' + input_file\n",
        "        csv_data = pd.read_csv(abs_path)\n",
        "        all_data = (np.array(csv_data))[:,1]\n",
        "        data1 = all_data[0:data_len]\n",
        "        get_single_tester_feats(data1, file_type)\n",
        "        data2 = all_data[data_len: ]\n",
        "        get_single_tester_feats(data2, file_type)\n",
        "        del data1, data2\n",
        "        gc.collect()\n",
        "\n",
        "def get_single_tester_feats(csv_data, file_type):\n",
        "    label = 1 if file_type=='stress' else 0\n",
        "    R_peak_locs = panTompkins(ECG=csv_data, fs=300, plot=0)\n",
        "    RR_interval_series = (np.diff(R_peak_locs)) / 300\n",
        "    timeDomainFeats = timeDomain(RR_interval_series)\n",
        "    freqDomainFeats = frequencyDomain(RR_interval_series)\n",
        "    fittingFeats = eclipseFittingMethod(RR_interval_series)\n",
        "    hraFeats = hraMethod(RR_interval_series)\n",
        "    feats_list = [val for val in timeDomainFeats.values()] + \\\n",
        "        [val for val in freqDomainFeats.values()] + \\\n",
        "        [val for val in fittingFeats.values()] + \\\n",
        "        [val for val in hraFeats.values()] + \\\n",
        "        [correlationCoef(RR_interval_series), label]\n",
        "    \n",
        "    if os.path.exists(output_file):\n",
        "        out = open(output_file, 'a', newline='')\n",
        "        csv_write = csv.writer(out,dialect='excel')\n",
        "        csv_write.writerow(feats_list)\n",
        "    else:    # 不存在文件则写入header+data\n",
        "        t_keys = [key for key in timeDomainFeats.keys()]\n",
        "        f_keys = [key for key in freqDomainFeats.keys()]\n",
        "        feats_header_list = [key for key in timeDomainFeats.keys()] + \\\n",
        "            [key for key in freqDomainFeats.keys()] + \\\n",
        "            [key for key in fittingFeats.keys()] + \\\n",
        "            [key for key in hraFeats.keys()] + \\\n",
        "            ['CorrCoef','label']\n",
        "        out = open(output_file, 'a', newline='')\n",
        "        csv_write = csv.writer(out,dialect='excel')\n",
        "        csv_write.writerow(feats_header_list)\n",
        "        csv_write.writerow(feats_list) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-271WB-FEi-E",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data_len = 90000\n",
        "\n",
        "for num in range(4, 19):\n",
        "    abs_path = os.getcwd() + '/Data/NO'  + str(num) + '/' + input_file_name\n",
        "    csv_data = pd.read_csv(abs_path)\n",
        "    all_data = (np.array(csv_data))[:,1]\n",
        "    data1 = all_data[0:data_len]    \n",
        "    R_peak_locs = panTompkins(ECG=data1, fs=300, plot=0)\n",
        "    RR_interval_series = (np.diff(R_peak_locs)) / 300\n",
        "    frequency_domain_features = get_frequency_domain_features(RR_interval_series*1000)\n",
        "    freqDomainFeats = frequencyDomain(RR_interval_series)\n",
        "    print(str(2*(num-4)+1) +':\\t'+ str(frequency_domain_features['lf_hf_ratio'])+'\\t'+str(freqDomainFeats['LF/HF']))\n",
        "    # ----------------------------------------------------------------------------------------------------------------------\n",
        "    data2 = all_data[data_len: ]\n",
        "    R_peak_locs = panTompkins(ECG=data2, fs=300, plot=0)\n",
        "    RR_interval_series = (np.diff(R_peak_locs)) / 300    \n",
        "    frequency_domain_features = get_frequency_domain_features(RR_interval_series*1000)\n",
        "    freqDomainFeats = frequencyDomain(RR_interval_series)\n",
        "    print(str(2*(num-4)+2) +':\\t'+ str(frequency_domain_features['lf_hf_ratio'])+'\\t'+str(freqDomainFeats['LF/HF']))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6FjBAQ-Z1w-F",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 函数\n"
      ]
    },
    {
      "metadata": {
        "id": "9KgJi-CwUI6J",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import gc\n",
        "import numpy as np\n",
        "from matplotlib import style\n",
        "from scipy import signal\n",
        "import matplotlib.pyplot as plt \n",
        "style.use('ggplot')\n",
        "\n",
        "def panTompkins(ECG, fs, plot = 1): \n",
        "    \"\"\"\n",
        "    Inputs:\n",
        "    - ECG   : [list] | [numpy.ndarray] of ECG samples\n",
        "    - fs    : [int] sampling frequency\n",
        "    - plot  : [1|0] optional plot of R-peak detections overlayed on ECG signal\n",
        "\n",
        "    Outputs:\n",
        "    - Rpeaks : [list] of integers indicating R peak sample number locations\n",
        "    \"\"\"    \n",
        "    # if type(ECG) == list or type(ECG) is np.ndarray:\n",
        "    #     ECG = np.array(ECG)             \n",
        "        \n",
        "    #Initialize\n",
        "    RRAVERAGE1 = []\n",
        "    RRAVERAGE2 = []\n",
        "    IWF_signal_peaks = []\n",
        "    IWF_noise_peaks = []\n",
        "    noise_peaks = []\n",
        "    ECG_bp_peaks = np.array([])\n",
        "    ECG_bp_signal_peaks = []\n",
        "    ECG_bp_noise_peaks = []\n",
        "    final_R_locs = []\n",
        "    T_wave_found = 0      \n",
        "    \n",
        "    #LOW PASS FILTERING\n",
        "    #Transfer function: H(z)=(1-z^-6)^2/(1-z^-1)^2\n",
        "    a = np.array([1, -2, 1])\n",
        "    b = np.array([1, 0, 0, 0, 0, 0, -2, 0, 0, 0, 0, 0, 1])   \n",
        "        \n",
        "    impulse = np.repeat(0., len(b)); impulse[0] = 1.    \n",
        "    impulse_response = signal.lfilter(b,a,impulse)\n",
        "    \n",
        "    #convolve ECG signal with impulse response\n",
        "    ECG_lp = np.convolve(impulse_response, ECG)\n",
        "    ECG_lp = ECG_lp / (max(abs(ECG_lp)))\n",
        "    delay = 12 #full convolution\n",
        "    \n",
        "    #HIGH PASS FILTERING\n",
        "    #Transfer function: H(z)=(-1+32z^-16+z^-32)/(1+z^-1)\n",
        "    a = np.array([1, -1])           \n",
        "    b = np.array([-1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
        "                  0, 0, 0, 32, -32, 0, 0, 0, 0, 0, 0, 0, 0,\n",
        "                  0, 0, 0, 0, 0, 0, -1])\n",
        "                  \n",
        "    impulse = np.repeat(0., len(b)); impulse[0] = 1.    \n",
        "    impulse_response = signal.lfilter(b,a,impulse)\n",
        "    \n",
        "    ECG_lp_hp = np.convolve(impulse_response, ECG_lp)\n",
        "    ECG_lp_hp = ECG_lp_hp/(max(abs(ECG_lp_hp)))\n",
        "    delay = delay + 32 \n",
        "    \n",
        "    #BAND PASS FILTER \n",
        "    nyq = fs / 2        \n",
        "    lowCut = 5 / nyq  #cut off frequencies are normalized from 0 to 1, where 1 is the Nyquist frequency\n",
        "    highCut = 15 / nyq\n",
        "    order = 5\n",
        "    b,a = signal.butter(order, [lowCut, highCut], btype = 'bandpass')\n",
        "    ECG_bp = signal.lfilter(b, a, ECG_lp_hp)\n",
        "    \n",
        "    #DIFFERENTIATION\n",
        "    #Transfer function: H(z)=(1/8T)(-z^-2-2z^-1+2z^1+z^2)\n",
        "    T = 1/fs\n",
        "    b = np.array([-1, -2, 0, 2, 1]) * (1 / (8 * T))\n",
        "    a = 1\n",
        "    #Note impulse response of the filter with a = [1] is b\n",
        "    ECG_deriv = np.convolve(ECG_bp, b)\n",
        "    delay = delay + 4 \n",
        "    \n",
        "    #SQUARING FUNCTION\n",
        "    ECG_squared = ECG_deriv ** 2\n",
        "    \n",
        "    #MOVING INTEGRATION WAVEFORM \n",
        "    N = int(np.ceil(0.150 * fs)) \n",
        "    ECG_movavg = np.convolve(ECG_squared,(1 / N) * np.ones((1, N))[0])\n",
        "    \n",
        "    #FUDICIAL MARK ON MOVING INTEGRATION WAVEFORM\n",
        "    peaks = findPeaks(ECG_movavg)\n",
        "    \n",
        "    #LEARNING PHASE 1   \n",
        "    #2 second initialize phase for MIW, 25% of max amplitude considered signal, 50% of mean signal considered noise\n",
        "    initializeTime = 2 * fs \n",
        "    SPKI = max(ECG_movavg[:initializeTime]) * 0.25 \n",
        "    NPKI = np.mean(ECG_movavg[:initializeTime]) * 0.5 \n",
        "    THRESHOLDI1 = NPKI + 0.25 * (SPKI-NPKI)\n",
        "    THRESHOLDI2 = 0.5 * THRESHOLDI1 \n",
        "    \n",
        "    #2 second initialize for filtered signal, 25% of max amplitude considered signal, 50% of mean signal considered noise\n",
        "    initializeTime = 2 * fs \n",
        "    SPKF = max(ECG_bp[:initializeTime]) * 0.25 \n",
        "    NPKF = np.mean(ECG_bp[:initializeTime]) * 0.5 \n",
        "    THRESHOLDF1 = NPKF + 0.25 * (SPKF-NPKF)\n",
        "    THRESHOLDF2 = 0.5 * THRESHOLDF1\n",
        "    \n",
        "    peaks = peaks[peaks > initializeTime] #ignore peaks that occur during initialization window\n",
        "\n",
        "    for c,peak in enumerate(peaks):                \n",
        "        #find corresponding peaks in filtered ECG using neighborhood search window +- 0.15 seconds       \n",
        "        searchInterval = int(np.round(0.15 * fs))\n",
        "        searchIndices = np.arange(peak - searchInterval, peak + searchInterval + 1, 1)        \n",
        "        #neighborhood search indices cannot be negative and cannot exceed length of filtered ECG\n",
        "        if searchIndices[0] >= 0 and all(searchIndices <= len(ECG_bp)):            \n",
        "             ECG_bp_peaks = np.append(ECG_bp_peaks, np.where(ECG_bp == max(ECG_bp[searchIndices-1]))[0][0])          \n",
        "        else:\n",
        "             ECG_bp_peaks = np.append(ECG_bp_peaks, np.where(ECG_bp == max(ECG_bp[searchIndices[0]:len(ECG_bp)-1])))\n",
        "        #LEARNING PHASE 2\n",
        "        if c > 0 and c < len(ECG_bp_peaks):                     \n",
        "            if c < 8:                \n",
        "                RRAVERAGE1_vec = np.diff(peaks[:c + 1]) / fs\n",
        "                RRAVERAGE1_mean = np.mean(RRAVERAGE1_vec)\n",
        "                RRAVERAGE1.append(RRAVERAGE1_mean) \n",
        "                \n",
        "                RR_LOW_LIMIT = 0.92 * RRAVERAGE1_mean\n",
        "                RR_HIGH_LIMIT = 1.16 * RRAVERAGE1_mean\n",
        "                RR_MISSED_LIMIT = 1.66 * RRAVERAGE1_mean                   \n",
        "            else:                \n",
        "                RRAVERAGE1_vec = np.diff(peaks[c - 8:c + 1]) / fs\n",
        "                RRAVERAGE1_mean = np.mean(RRAVERAGE1_vec)\n",
        "                RRAVERAGE1.append(RRAVERAGE1_mean) \n",
        "    \n",
        "                for rr in np.arange(0, len(RRAVERAGE1_vec)):\n",
        "                    if RRAVERAGE1_vec[rr] > RR_LOW_LIMIT and RRAVERAGE1_vec[rr] < RR_HIGH_LIMIT:                              \n",
        "                        RRAVERAGE2.append(RRAVERAGE1_vec[rr])                                     \n",
        "                        if len(RRAVERAGE2) > 8:\n",
        "                            del RRAVERAGE2[:len(RRAVERAGE2) - 8]\n",
        "    \n",
        "                if len(RRAVERAGE2) == 8:\n",
        "                    RR_LOW_LIMIT = 0.92 * np.mean(RRAVERAGE2)        \n",
        "                    RR_HIGH_LIMIT = 1.16 * np.mean(RRAVERAGE2)\n",
        "                    RR_MISSED_LIMIT = 1.66 * np.mean(RRAVERAGE2)\n",
        "            #If irregular heart beat detected in previous 9 beats, lower signal thresholds by half to increase detection sensitivity            \n",
        "            current_RR_movavg = RRAVERAGE1[-1] \n",
        "            if current_RR_movavg < RR_LOW_LIMIT or current_RR_movavg > RR_MISSED_LIMIT: \n",
        "                #MIW thresholds        \n",
        "                THRESHOLDI1 = 0.5 * THRESHOLDI1\n",
        "                THRESHOLDI2 = 0.5 * THRESHOLDI1\n",
        "                #Filtered ECG thresholds\n",
        "                THRESHOLDF1 = 0.5 * THRESHOLDF1\n",
        "                THRESHOLDF2 = 0.5 * THRESHOLDF1\n",
        "               \n",
        "            #Search back triggered if current RR interval is greater than RR_MISSED_LIMIT\n",
        "            currentRRint = RRAVERAGE1_vec[-1]\n",
        "            if currentRRint > RR_MISSED_LIMIT:  \n",
        "                SBinterval = int(np.round(currentRRint * fs))\n",
        "                #find local maximum in the search back interval between signal and noise thresholds                        \n",
        "                SBdata_IWF = ECG_movavg[peak - SBinterval + 1:peak + 1]               \n",
        "                \n",
        "                SBdata_IWF_filtered = np.where((SBdata_IWF > THRESHOLDI1))[0]\n",
        "                if SBdata_IWF_filtered != []:\n",
        "                    SBdata_max_loc = np.where(SBdata_IWF == max(SBdata_IWF[SBdata_IWF_filtered]))[0][0]\n",
        "\n",
        "                if len(SBdata_IWF_filtered) > 0:   \n",
        "                    SB_IWF_loc = peak - SBinterval + 1 + SBdata_max_loc\n",
        "                    IWF_signal_peaks.append(SB_IWF_loc) \n",
        "                    #update signal and noise thresholds\n",
        "                    SPKI = 0.25 * ECG_movavg[SB_IWF_loc] + 0.75 * SPKI                         \n",
        "                    THRESHOLDI1 = NPKI + 0.25 * (SPKI - NPKI)\n",
        "                    THRESHOLDI2 = 0.5 * THRESHOLDI1               \n",
        "                    #finding corresponding search back peak in ECG bandpass using 0.15 s neighborhood search window\n",
        "                    if SB_IWF_loc < len(ECG_bp):\n",
        "                        SBdata_ECGfilt = ECG_bp[SB_IWF_loc - round(0.15 * fs): SB_IWF_loc]                    \n",
        "                        SBdata_ECGfilt_filtered = np.where((SBdata_ECGfilt > THRESHOLDF1))[0]\n",
        "                        SBdata_max_loc2 = np.where(SBdata_ECGfilt == max(SBdata_ECGfilt[SBdata_ECGfilt_filtered]))[0][0]\n",
        "                                     \n",
        "                    else:\n",
        "                        SBdata_ECGfilt = ECG_bp[SB_IWF_loc - round(0.15 * fs):]\n",
        "                        SBdata_ECGfilt_filtered = np.where((SBdata_ECGfilt > THRESHOLDF1))[0]\n",
        "                        SBdata_max_loc2 = np.where(SBdata_ECGfilt == max(SBdata_ECGfilt[SBdata_ECGfilt_filtered]))[0][0]\n",
        "\n",
        "                            \n",
        "                    if ECG_bp[SB_IWF_loc - round(0.15 * fs) + SBdata_max_loc2] > THRESHOLDF2: #QRS complex detected in filtered ECG\n",
        "                        #update signal and noise thresholds                                                          \n",
        "                        SPKF = 0.25 * ECG_bp[SB_IWF_loc - round(0.15 * fs) + SBdata_max_loc2] + 0.75 * SPKF                            \n",
        "                        THRESHOLDF1 = NPKF + 0.25 * (SPKF - NPKF)\n",
        "                        THRESHOLDF2 = 0.5 * THRESHOLDF1                            \n",
        "                        ECG_bp_signal_peaks.append(SB_IWF_loc - round(0.15 * fs) + SBdata_max_loc2)                                                 \n",
        "    \n",
        "            #T-WAVE AND QRS DISRCIMINATION    \n",
        "            if ECG_movavg[peak] >= THRESHOLDI1: \n",
        "                if currentRRint > 0.20 and currentRRint < 0.36 and c > 0: \n",
        "                    #Slope of current waveform (possible T wave)\n",
        "                    #mean width of QRS complex: 0.06 - 0.10 sec         \n",
        "                    maxSlope_current = max(np.diff(ECG_movavg[peak - round(fs * 0.075):peak + 1]))\n",
        "                    #slope of the waveform (most likely QRS) that preceeded it\n",
        "                    maxSlope_past = max(np.diff(ECG_movavg[peaks[c - 1] - round(fs * 0.075): peaks[c - 1] + 1]))\n",
        "                    if maxSlope_current < 0.5 * maxSlope_past: #T-wave found                        \n",
        "                        T_wave_found = 1                \n",
        "                        #keep track of peaks marked as 'noise'\n",
        "                        IWF_noise_peaks.append(peak)                \n",
        "                        #update Noise levels\n",
        "                        NPKI = 0.125 * ECG_movavg[peak] + 0.875 * NPKI                                            \n",
        "                               \n",
        "                if not T_wave_found: #current peak is a signal peak                    \n",
        "                    IWF_signal_peaks.append(peak)\n",
        "                    #adjust signal levels\n",
        "                    SPKI = 0.125 * ECG_movavg[peak]  + 0.875 * SPKI\n",
        "                    #check if corresponding peak in filtered ECG is also a signal peak                        \n",
        "                    if ECG_bp_peaks[c] > THRESHOLDF1:                                            \n",
        "                        SPKF = 0.125 * ECG_bp[c] + 0.875 * SPKF \n",
        "                        ECG_bp_signal_peaks.append(ECG_bp_peaks[c])                             \n",
        "                    else:\n",
        "                        ECG_bp_noise_peaks.append(ECG_bp_peaks[c])\n",
        "                        NPKF = 0.125 * ECG_bp[c] + 0.875 * NPKF                   \n",
        "                                        \n",
        "            elif ECG_movavg[peak] > THRESHOLDI1 and ECG_movavg[peak] < THRESHOLDI2:\n",
        "                #update noise thresholds\n",
        "                NPKI = 0.125 * ECG_movavg[peak]  + 0.875 * NPKI  \n",
        "                NPKF = 0.125 * ECG_bp[c] + 0.875 * NPKF\n",
        "                    \n",
        "            elif ECG_movavg[peak] < THRESHOLDI1:\n",
        "                #update noise thresholds\n",
        "                noise_peaks.append(peak)\n",
        "                NPKI = 0.125 * ECG_movavg[peak]  + 0.875 * NPKI            \n",
        "                ECG_bp_noise_peaks.append(ECG_bp_peaks[c])                       \n",
        "                NPKF = 0.125 * ECG_bp[c] + 0.875 * NPKF\n",
        "        else:\n",
        "            if ECG_movavg[peak] >= THRESHOLDI1: #first peak is a signal peak\n",
        "                IWF_signal_peaks.append(peak) \n",
        "                #update signal  thresholds\n",
        "                SPKI = 0.125 * ECG_movavg[peak]  + 0.875 * SPKI\n",
        "                if ECG_bp_peaks[c] > THRESHOLDF1:                                            \n",
        "                    SPKF = 0.125 * ECG_bp[c] + 0.875 * SPKF \n",
        "                    ECG_bp_signal_peaks.append(ECG_bp_peaks[c])                             \n",
        "                else:\n",
        "                    ECG_bp_noise_peaks.append(ECG_bp_peaks[c])\n",
        "                    NPKF = 0.125 * ECG_bp[c] + 0.875 * NPKF                                    \n",
        "                \n",
        "            elif ECG_movavg[peak] > THRESHOLDI2 and ECG_movavg[peak] < THRESHOLDI1:\n",
        "                #update noise thresholds\n",
        "                NPKI = 0.125 * ECG_movavg[peak]  + 0.875 * NPKI  \n",
        "                NPKF = 0.125 * ECG[c] + 0.875 * NPKF\n",
        "                                    \n",
        "            elif ECG_movavg[peak] < THRESHOLDI2:\n",
        "                #update noise thresholds\n",
        "                noise_peaks.append(peak)\n",
        "                NPKI = 0.125 * ECG_movavg[peak]  + 0.875 * NPKI            \n",
        "                ECG_bp_noise_peaks.append(ECG_bp_peaks[c])                       \n",
        "                NPKF = 0.125 * ECG_bp[c] + 0.875 * NPKF       \n",
        "            \n",
        "                    \n",
        "        #reset \n",
        "        T_wave_found = 0                \n",
        "            \n",
        "        #update thresholds\n",
        "        THRESHOLDI1 = NPKI + 0.25 * (SPKI - NPKI)\n",
        "        THRESHOLDI2 = 0.5 * THRESHOLDI1 \n",
        "            \n",
        "        THRESHOLDF1 = NPKF + 0.25 * (SPKF - NPKF)\n",
        "        THRESHOLDF2 = 0.5 * THRESHOLDF1\n",
        "    \n",
        "    #adjust for filter delays\n",
        "    ECG_R_locs = [int(i - delay) for i in ECG_bp_signal_peaks]\n",
        "    ECG_R_locs = np.unique(ECG_R_locs)\n",
        "    \n",
        "    #neighborhood search in raw ECG signal for increase accuracy of R peak detection    \n",
        "    for i in ECG_R_locs:\n",
        "        ECG = np.array(ECG)\n",
        "        searchInterval = int(np.round(0.02 * fs))\n",
        "        searchIndices = np.arange(i - searchInterval, i + searchInterval + 1, 1)\n",
        "        searchIndices = [i.item()-2 for i in searchIndices] #convert to native Python int        \n",
        "        final_R_locs.append(np.where(ECG[searchIndices] == max(ECG[searchIndices]))[0][0] + searchIndices[0])\n",
        "    \n",
        "    #plot ECG signal with R peaks marked\n",
        "    if plot == 1:\n",
        "        samples = np.arange(0, len(ECG))\n",
        "        plt.plot(samples, ECG, c = 'b')        \n",
        "        plt.scatter(final_R_locs, ECG[final_R_locs], c = 'r', s = 30)\n",
        "        plt.xlabel('Sample')\n",
        "        plt.ylabel('ECG')\n",
        "        plt.show()\n",
        "    else:\n",
        "        pass\n",
        "\n",
        "    for x in locals().keys():\n",
        "        del locals()[x]\n",
        "    gc.collect()\n",
        "\n",
        "    return final_R_locs\n",
        "                    \n",
        "def findPeaks(ECG_movavg):\n",
        "    \"\"\"finds peaks in Integration Waveform by smoothing, locating zero crossings, and moving average amplitude thresholding\"\"\"\n",
        "    #smoothing\n",
        "    N = 15\n",
        "    ECG_movavg_smooth = np.convolve(ECG_movavg, np.ones((N,)) / N, mode = 'same')    \n",
        "    #signal derivative    \n",
        "    sigDeriv = np.diff(ECG_movavg_smooth)     \n",
        "    #find location of zero-crossings\n",
        "    zeroCross = []\n",
        "    for i,c in enumerate(np.arange(len(sigDeriv)-1)):\n",
        "        if sigDeriv[i] > 0 and sigDeriv[i + 1] < 0:\n",
        "            zeroCross.append(c)           \n",
        "    \n",
        "    return np.array(zeroCross) \n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kHQSVDXHApzt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Wed Mar  1 13:59:24 2017\n",
        "\n",
        "@author: picku\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def timeDomain(NN):\n",
        "    \n",
        "    L = len(NN)    \n",
        "    ANN = np.mean(NN)\n",
        "    SDNN = np.std(NN)\n",
        "    SDSD = np.std(np.diff(NN))    \n",
        "    NN50 = sum(np.abs(np.diff(NN)) > 0.05)\n",
        "    pNN50 = NN50/L    \n",
        "    NN20 = sum(np.abs(np.diff(NN)) > 0.02)\n",
        "    pNN20 = NN20/L\n",
        "    rMSSD = np.sqrt((1/L) * sum(np.diff(NN) ** 2))        \n",
        "    MedianNN = np.median(NN)\n",
        "    \n",
        "    timeDomainFeats = {'ANN': ANN, 'SDNN': SDNN,\n",
        "                       'SDSD': SDSD, 'NN50': NN50,\n",
        "                       'pNN50': pNN50, 'NN20': NN20,\n",
        "                       'pNN20': pNN20, 'rMSSD': rMSSD,\n",
        "                       'MedianNN':MedianNN}\n",
        "                       \n",
        "    return timeDomainFeats"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OC952XE11EJh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy import interpolate, signal\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import style\n",
        "style.use('ggplot')\n",
        "import matplotlib.patches as mpatches\n",
        "from collections import OrderedDict\n",
        "\n",
        "def frequencyDomain(RRints, band_type = None, lf_bw = 0.11, hf_bw = 0.1, plot = 0):\n",
        "    #Remove ectopic beats\n",
        "    #RR intervals differing by more than 20% from the one proceeding it are removed\n",
        "    NNs = []\n",
        "    for c, rr in enumerate(RRints):        \n",
        "        if abs(rr - RRints[c-1]) <= 0.20 * RRints[c-1]:\n",
        "            NNs.append(rr)\n",
        "      \n",
        "    #Resample @ 4 Hz\n",
        "    fsResamp = 4   \n",
        "    tmStamps = np.cumsum(NNs) #in seconds \n",
        "    f = interpolate.interp1d(tmStamps, NNs, 'cubic')\n",
        "    tmInterp = np.arange(tmStamps[0], tmStamps[-1], 1/fsResamp)\n",
        "    RRinterp = f(tmInterp)          \n",
        "    \n",
        "    #Remove DC component     \n",
        "    RRseries = RRinterp - np.mean(RRinterp)\n",
        "        \n",
        "    #Pwelch w/ zero pad     \n",
        "    fxx, pxx = signal.welch(RRseries, fsResamp, nfft = 2**14, window = 'hann')    \n",
        "    \n",
        "    vlf= (0.003, 0.04)\n",
        "    lf = (0.04, 0.15)\n",
        "    hf = (0.15, 0.4)\n",
        "    \n",
        "    plot_labels = ['VLF', 'LF', 'HF']\n",
        "        \n",
        "    if band_type == 'adapted':     \n",
        "            \n",
        "        vlf_peak = fxx[np.where(pxx == np.max(pxx[np.logical_and(fxx >= vlf[0], fxx < vlf[1])]))[0][0]] \n",
        "        lf_peak = fxx[np.where(pxx == np.max(pxx[np.logical_and(fxx >= lf[0], fxx < lf[1])]))[0][0]]\n",
        "        hf_peak = fxx[np.where(pxx == np.max(pxx[np.logical_and(fxx >= hf[0], fxx < hf[1])]))[0][0]]\n",
        "    \n",
        "        peak_freqs =  (vlf_peak, lf_peak, hf_peak) \n",
        "            \n",
        "        hf = (peak_freqs[2] - hf_bw/2, peak_freqs[2] + hf_bw/2)\n",
        "        lf = (peak_freqs[1] - lf_bw/2, peak_freqs[1] + lf_bw/2)   \n",
        "        vlf = (0.003, lf[0])\n",
        "        \n",
        "        if lf[0] < 0:\n",
        "            print('***Warning***: Adapted LF band lower bound spills into negative frequency range')\n",
        "            print('Lower thresold of LF band has been set to zero')\n",
        "            print('Adjust LF and HF bandwidths accordingly')\n",
        "            lf = (0, lf[1])        \n",
        "            vlf = (0, 0)\n",
        "        elif hf[0] < 0:\n",
        "            print('***Warning***: Adapted HF band lower bound spills into negative frequency range')\n",
        "            print('Lower thresold of HF band has been set to zero')\n",
        "            print('Adjust LF and HF bandwidths accordingly')\n",
        "            hf = (0, hf[1])        \n",
        "            lf = (0, 0)        \n",
        "            vlf = (0, 0)\n",
        "            \n",
        "        plot_labels = ['Adapted_VLF', 'Adapted_LF', 'Adapted_HF']\n",
        "\n",
        "    df = fxx[1] - fxx[0]\n",
        "    vlf_power = np.trapz(pxx[np.logical_and(fxx >= vlf[0], fxx < vlf[1])], dx = df)      \n",
        "    lf_power = np.trapz(pxx[np.logical_and(fxx >= lf[0], fxx < lf[1])], dx = df)            \n",
        "    hf_power = np.trapz(pxx[np.logical_and(fxx >= hf[0], fxx < hf[1])], dx = df)             \n",
        "    totalPower = vlf_power + lf_power + hf_power\n",
        "    \n",
        "    #Normalize and take log\n",
        "    vlf_NU_log = np.log((vlf_power / (totalPower - vlf_power)) + 1)\n",
        "    lf_NU_log = np.log((lf_power / (totalPower - vlf_power)) + 1)\n",
        "    hf_NU_log = np.log((hf_power / (totalPower - vlf_power)) + 1)\n",
        "    lfhfRation_log = np.log((lf_power / hf_power) + 1)   \n",
        "    \n",
        "    freqDomainFeats = {'VLF_Power': vlf_NU_log, 'LF_Power': lf_NU_log,\n",
        "                       'HF_Power': hf_NU_log, 'LF/HF': lfhfRation_log}\n",
        "                       \n",
        "    if plot == 1:\n",
        "        #Plot option\n",
        "        freq_bands = {'vlf': vlf, 'lf': lf, 'hf': hf}\n",
        "        freq_bands = OrderedDict(sorted(freq_bands.items(), key=lambda t: t[0]))\n",
        "        colors = ['lightsalmon', 'lightsteelblue', 'darkseagreen']\n",
        "        fig, ax = plt.subplots(1)\n",
        "        ax.plot(fxx, pxx, c = 'grey')\n",
        "        plt.xlim([0, 0.40])\n",
        "        plt.xlabel(r'Frequency $(Hz)$')\n",
        "        plt.ylabel(r'PSD $(s^2/Hz$)')\n",
        "        \n",
        "        for c, key in enumerate(freq_bands):\n",
        "            ax.fill_between(fxx[min(np.where(fxx >= freq_bands[key][0])[0]): max(np.where(fxx <= freq_bands[key][1])[0])],\n",
        "                            pxx[min(np.where(fxx >= freq_bands[key][0])[0]): max(np.where(fxx <= freq_bands[key][1])[0])],\n",
        "                            0, facecolor = colors[c])\n",
        "            \n",
        "        patch1 = mpatches.Patch(color = colors[0], label = plot_labels[2])\n",
        "        patch2 = mpatches.Patch(color = colors[1], label = plot_labels[1])\n",
        "        patch3 = mpatches.Patch(color = colors[2], label = plot_labels[0])\n",
        "        plt.legend(handles = [patch1, patch2, patch3])\n",
        "        plt.show()\n",
        "\n",
        "    return freqDomainFeats"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Op81o1Z2_D6R",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    }
  ]
}