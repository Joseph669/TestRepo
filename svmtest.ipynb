{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "svmtest.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "68R0dZ1lsxRW",
        "_OsCL9tuQbVD"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Joseph669/TestRepo/blob/master/svmtest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "DSo2hbAQ-_Gx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        },
        "outputId": "5faf3690-d25c-4c4d-bf2f-43b77da3de77"
      },
      "cell_type": "code",
      "source": [
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}\n",
        "\n",
        "# 挂载Google Drive\n",
        "!mkdir -p drive\n",
        "!google-drive-ocamlfuse drive\n",
        "\n",
        "import os\n",
        "os.chdir(\"/content/drive/HRV\")\n",
        "!ls"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "E: Package 'python-software-properties' has no installation candidate\n",
            "Selecting previously unselected package google-drive-ocamlfuse.\n",
            "(Reading database ... 131304 files and directories currently installed.)\n",
            "Preparing to unpack .../google-drive-ocamlfuse_0.7.3-0ubuntu3~ubuntu18.04.1_amd64.deb ...\n",
            "Unpacking google-drive-ocamlfuse (0.7.3-0ubuntu3~ubuntu18.04.1) ...\n",
            "Setting up google-drive-ocamlfuse (0.7.3-0ubuntu3~ubuntu18.04.1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "··········\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "Please enter the verification code: Access token retrieved correctly.\n",
            "Data\t\t    multiScaleEntropy.py      subtraction.csv\n",
            "DFA.py\t\t    new_get_train_feats.py    test_feats_data.csv\n",
            "feats_data.csv\t    new_train_feats_data.csv  test_fig_data.csv\n",
            "frequencyDomain.py  panTompkins.py\t      timeDomain.py\n",
            "get_test_feats.py   poincare.py\t\t      train_fig_data.csv\n",
            "get_train_feats.py  __pycache__\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "pGMwIZxeui8_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pwd\n",
        "import os\n",
        "os.chdir(\"/content/drive/HRV\")\n",
        "!ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "C2JtmVYekFOK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100
        },
        "outputId": "3807f9a4-0a21-4067-a311-ef2d66727d23"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "iris = load_iris()\n",
        "X_train,X_test,y_train,y_test = train_test_split(iris.data,iris.target,random_state=0)\n",
        "print(\"Size of training set:{} size of testing set:{}\".format(X_train.shape[0],X_test.shape[0]))\n",
        "\n",
        "X_trainval,X_test,y_trainval,y_test = train_test_split(iris.data,iris.target,random_state=0)\n",
        "X_train,X_val,y_train,y_val = train_test_split(X_trainval,y_trainval,random_state=1)\n",
        "print(\"Size of training set:{} size of validation set:{} size of teseting set:{}\".format(X_train.shape[0],X_val.shape[0],X_test.shape[0]))\n",
        "\n",
        "best_score = 0.0\n",
        "for gamma in [0.001,0.01,0.1,1,10,100]:\n",
        "    for C in [0.001,0.01,0.1,1,10,100]:\n",
        "        svm = SVC(gamma=gamma,C=C)\n",
        "        scores = cross_val_score(svm,X_trainval,y_trainval,cv=5) #5折交叉验证\n",
        "        score = scores.mean() #取平均数\n",
        "        if score > best_score:\n",
        "            best_score = score\n",
        "            best_parameters = {\"gamma\":gamma,\"C\":C}\n",
        "svm = SVC(**best_parameters)\n",
        "svm.fit(X_trainval,y_trainval)\n",
        "test_score = svm.score(X_test,y_test)\n",
        "print(\"Best score on validation set:{:.2f}\".format(best_score))\n",
        "print(\"Best parameters:{}\".format(best_parameters))\n",
        "print(\"Score on testing set:{:.2f}\".format(test_score))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of training set:112 size of testing set:38\n",
            "Size of training set:84 size of validation set:28 size of teseting set:38\n",
            "Best score on validation set:0.97\n",
            "Best parameters:{'gamma': 0.01, 'C': 100}\n",
            "Score on testing set:0.97\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "nQnYGM61lEjk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "16d1fa51-7121-460c-ebc2-a9b3f8c7028a"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "#把要调整的参数以及其候选值 列出来；\n",
        "param_grid = {\"gamma\":[0.001,0.01,0.1,1,10,100],\n",
        "             \"C\":[0.001,0.01,0.1,1,10,100]}\n",
        "print(\"Parameters:{}\".format(param_grid))\n",
        "\n",
        "grid_search = GridSearchCV(SVC(),param_grid,cv=5) #实例化一个GridSearchCV类\n",
        "X_train,X_test,y_train,y_test = train_test_split(iris.data,iris.target,random_state=10)\n",
        "grid_search.fit(X_train,y_train) #训练，找到最优的参数，同时使用最优的参数实例化一个新的SVC estimator。\n",
        "print(\"Test set score:{:.2f}\".format(grid_search.score(X_test,y_test)))\n",
        "print(\"Best parameters:{}\".format(grid_search.best_params_))\n",
        "print(\"Best score on train set:{:.2f}\".format(grid_search.best_score_))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Parameters:{'gamma': [0.001, 0.01, 0.1, 1, 10, 100], 'C': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
            "Test set score:0.97\n",
            "Best parameters:{'C': 10, 'gamma': 0.1}\n",
            "Best score on train set:0.98\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
            "  DeprecationWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "tPi8D58j2oct",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#from get_train_feats import get_train_feats\n",
        "from get_test_feats import get_test_feats\n",
        "from new_get_train_feats import new_get_train_feats\n",
        "\n",
        "new_get_train_feats() \n",
        "#get_train_feats()  # video1,3  NO4-NO18\n",
        "get_test_feats()   # video2    NO4-NO18\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EMUljjYocNka",
        "colab_type": "code",
        "outputId": "3812bcd4-1d40-4900-d1b0-6840582be419",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 150
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.svm import SVC\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection  import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib as plt \n",
        "import pandas as pd\n",
        "from random import sample\n",
        "\n",
        "file_name = 'new_train_feats_data.csv'\n",
        "#file_name = 'test_feats_data.csv'\n",
        "# feats_tuple     = sample(range(0,10), 3)\n",
        "feats_tuple     =  [4,6]\n",
        "\n",
        "file_data = np.array(pd.read_csv(file_name))\n",
        "feats     = file_data[:, feats_tuple]\n",
        "#feats     = preprocessing.StandardScaler().fit_transform(feats)\n",
        "label     = file_data[:, -1]\n",
        "x_train,x_test,y_train,y_test = train_test_split(feats, label, test_size=0.1, random_state=110)\n",
        "clf = SVC(C=0.5, coef0=0, kernel='rbf', gamma='auto').fit(x_train, y_train) #调参\n",
        "\n",
        "\n",
        "print('----------------------------------------------------------')        \n",
        "print('----------------------------------------------------------')\n",
        "print('train accuracy:' + str(clf.score(x_train, y_train)))\n",
        "print(confusion_matrix(y_train, clf.predict(x_train)))\n",
        "print('test accuracy:' + str(clf.score(x_test, y_test)))\n",
        "print(confusion_matrix(y_test, clf.predict(x_test)))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------\n",
            "----------------------------------------------------------\n",
            "train accuracy:0.6\n",
            "[[46  4]\n",
            " [36 14]]\n",
            "test accuracy:0.6666666666666666\n",
            "[[6 0]\n",
            " [4 2]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6dzXKSZug-tE",
        "colab_type": "code",
        "outputId": "2c571310-c138-4a76-b4fd-082224e130e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 150
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.svm import SVC\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection  import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib as plt \n",
        "import pandas as pd\n",
        "from random import sample\n",
        "\n",
        "# feats_tuple     = sample(range(0,10), 3)\n",
        "feats_tuple     = (10,20)\n",
        "\n",
        "\n",
        "file_name = 'new_train_feats_data.csv'\n",
        "file_data = np.array(pd.read_csv(file_name))\n",
        "x_train     = file_data[:, feats_tuple]\n",
        "#x_train     = preprocessing.StandardScaler().fit_transform(x_train)\n",
        "y_train     = file_data[:, -1]\n",
        "#x_train,x_test,y_train,y_test = train_test_split(x_train, y_train, test_size=0.3, random_state=2)\n",
        "\n",
        "file_name = 'test_feats_data.csv'\n",
        "file_data = np.array(pd.read_csv(file_name))\n",
        "x_test     = file_data[:, feats_tuple]\n",
        "#x_test     = preprocessing.StandardScaler().fit_transform(x_test)\n",
        "y_test     = file_data[:, -1]\n",
        "\n",
        "clf = SVC(C=100, coef0=0, kernel='rbf', gamma='auto').fit(x_train, y_train) #调参\n",
        "\n",
        "\n",
        "print('----------------------------------------------------------')        \n",
        "print('----------------------------------------------------------')\n",
        "print('train accuracy:' + str(clf.score(x_train, y_train)))\n",
        "print(confusion_matrix(y_train, clf.predict(x_train)))\n",
        "print('test accuracy:' + str(clf.score(x_test, y_test)))\n",
        "print(confusion_matrix(y_test, clf.predict(x_test)))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------\n",
            "----------------------------------------------------------\n",
            "train accuracy:0.5446428571428571\n",
            "[[35 21]\n",
            " [30 26]]\n",
            "test accuracy:0.37735849056603776\n",
            "[[ 9 18]\n",
            " [15 11]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "HuYVHxwW4ip_",
        "colab_type": "code",
        "outputId": "d6bb2116-39eb-4229-81c2-c6ab7a62a58a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 133
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier \n",
        "import numpy as np\n",
        "from sklearn.svm import SVC\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection  import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib as plt \n",
        "import pandas as pd\n",
        "from random import sample\n",
        "\n",
        "#feats_tuple     = sample(range(0,10), 3)\n",
        "#feats_tuple     =(0,1,4,10,16)\n",
        "#feats_tuple = [1,  2,  3,  7, 16]\n",
        "feats_tuple = [0, 3, 4, 5, 6]\n",
        "file_name = 'test_feats_data.csv'\n",
        "file_data = np.array(pd.read_csv(file_name))\n",
        "print(file_data.shape)\n",
        "x_train     = file_data[:, feats_tuple]\n",
        "#x_train     = preprocessing.StandardScaler().fit_transform(x_train)\n",
        "y_train     = file_data[:, -1]\n",
        "\n",
        "file_name = 'test_feats_data.csv'\n",
        "file_data = np.array(pd.read_csv(file_name))\n",
        "x_test     = file_data[:, feats_tuple]\n",
        "#x_test     = preprocessing.StandardScaler().fit_transform(x_test)\n",
        "y_test     = file_data[:, -1]\n",
        "x_train,x_test,y_train,y_test = train_test_split(x_train, y_train, test_size=0.1, random_state=2)\n",
        "#knn = KNeighborsClassifier().fit(x_train, y_train)\n",
        "knn = SVC(C=1, coef0=0, kernel='rbf', gamma='auto').fit(x_train, y_train) #调参\n",
        "\n",
        "y_predict = knn.predict(x_test) \n",
        "#probility=knn.predict_proba(x_test)  \n",
        "#neighborpoint=knn.kneighbors(x_test[-1].reshape(1, -1),5,False)\n",
        "score=knn.score(x_test,y_test,sample_weight=None)\n",
        "print('train Accuracy:'+str(knn.score(x_train,y_train,sample_weight=None)))\n",
        "print(confusion_matrix(y_train, knn.predict(x_train)))\n",
        "print('test Accuracy:'+str(score))\n",
        "print(confusion_matrix(y_test, knn.predict(x_test)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(53, 22)\n",
            "train Accuracy:1.0\n",
            "[[22  0]\n",
            " [ 0 25]]\n",
            "test Accuracy:0.16666666666666666\n",
            "[[0 5]\n",
            " [0 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "OB9ViSIG4uVO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.svm import SVC\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection  import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib as plt \n",
        "import pandas as pd\n",
        "from random import sample\n",
        "from itertools import combinations\n",
        "\n",
        "\n",
        "file_name = 'train_feats_data.csv'\n",
        "file_name = 'test_feats_data.csv'\n",
        "file_data = np.array(pd.read_csv(file_name))\n",
        "# feats_tuple     = sample(range(0,10), 3)\n",
        "feats_tuple     = (0,1,4, 7,12)\n",
        "\n",
        "def change_feats(feats_tuple):    \n",
        "    feats     = file_data[:, feats_tuple]\n",
        "    #feats     = preprocessing.StandardScaler().fit_transform(feats)\n",
        "    label     = file_data[:, -1]\n",
        "    x_train,x_test,y_train,y_test = train_test_split(feats, label, test_size=0.3, random_state=2)\n",
        "    clf = SVC(C=1000, coef0=0, kernel='rbf', gamma='auto').fit(x_train, y_train) #调参\n",
        "    # acc_dict[str(feats)] =  str(clf.score(x_train, y_train)) + ', ' + str(clf.score(x_test, y_test))\n",
        "    print('-------------------'+str(feats_tuple)+'--------------------------')\n",
        "    print('train accuracy:' + str(clf.score(x_train, y_train)))\n",
        "    print(confusion_matrix(y_train, clf.predict(x_train)))\n",
        "    print('test accuracy:' + str(clf.score(x_test, y_test)))\n",
        "    print(confusion_matrix(y_test, clf.predict(x_test)))\n",
        "\n",
        "    \n",
        "for feats in  itertools.combinations(feats_tuple, 4):\n",
        "    change_feats(feats)\n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "C_XWHlBZufob",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.svm import SVC\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection  import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib as plt \n",
        "import pandas as pd\n",
        "from random import sample\n",
        "from itertools import combinations\n",
        "\n",
        "\n",
        "file_name = 'new_train_feats_data.csv'\n",
        "#file_name = 'test_feats_data.csv'\n",
        "file_data = np.array(pd.read_csv(file_name))\n",
        "\n",
        "\n",
        "def change_data(feats_tuple, input_row=None): \n",
        "    if input_row:\n",
        "        global file_data\n",
        "        data_tuple = np.array(input_row)\n",
        "        data_tuple = np.concatenate((data_tuple, data_tuple+15, data_tuple+30, data_tuple+45,\n",
        "                                    data_tuple+60, data_tuple+75, data_tuple+90, data_tuple+105))\n",
        "        select_file_data = file_data[data_tuple, :]\n",
        "        feats     = select_file_data[:, feats_tuple]\n",
        "        label     = select_file_data[:, -1]\n",
        "        print('-------------------'+str(input_row)+'--------------------------')\n",
        "    else:\n",
        "        feats     = file_data[:, feats_tuple]\n",
        "        label     = file_data[:, -1]\n",
        "    #feats     = preprocessing.StandardScaler().fit_transform(feats)\n",
        "    #x_train,x_test,y_train,y_test = train_test_split(feats, label, test_size=0.3, random_state=2)\n",
        "    #clf = SVC(C=1000, coef0=0, kernel='rbf', gamma='auto').fit(x_train, y_train) #调参\n",
        "    # acc_dict[str(feats)] =  str(clf.score(x_train, y_train)) + ', ' + str(clf.score(x_test, y_test))\n",
        "    #file_name = 'new_train_feats_data.csv'\n",
        "    #file_data = np.array(pd.read_csv(file_name))\n",
        "    x_train     = file_data[:, feats_tuple]\n",
        "    x_train     = preprocessing.StandardScaler().fit_transform(x_train)\n",
        "    y_train     = file_data[:, -1]\n",
        "\n",
        "    file_name = 'test_feats_data.csv'\n",
        "    file_data = np.array(pd.read_csv(file_name))\n",
        "    x_test     = file_data[:, feats_tuple]\n",
        "    x_test     = preprocessing.StandardScaler().fit_transform(x_test)\n",
        "    y_test     = file_data[:, -1]\n",
        "    clf = SVC(C=1000, coef0=0, kernel='rbf', gamma='auto').fit(x_train, y_train) #调参\n",
        "    #print('-------------------'+str(feats_tuple)+'--------------------------')\n",
        "    print('train accuracy:' + str(clf.score(x_train, y_train)))\n",
        "    print(confusion_matrix(y_train, clf.predict(x_train)))\n",
        "    print('test accuracy:' + str(clf.score(x_test, y_test)))\n",
        "    print(confusion_matrix(y_test, clf.predict(x_test)))\n",
        "\n",
        "# feats_tuple     = sample(range(0,10), 3)\n",
        "feats_tuple     = (0,1,4, 7,12)    \n",
        "#for feats in  itertools.combinations(feats_tuple, 4):\n",
        "#    change_feats(feats)\n",
        "data_tuple = range(0,15)    \n",
        "for data_comb in  itertools.combinations(data_tuple, 12):\n",
        "    change_data(feats_tuple,data_comb)\n",
        "\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vKOVX25t7cwr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.svm import SVC\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection  import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib as plt \n",
        "import pandas as pd\n",
        "from random import sample\n",
        "from itertools import combinations\n",
        "\n",
        "\n",
        "file_name = 'train_feats_data.csv'\n",
        "file_name = 'test_feats_data.csv'\n",
        "file_data = np.array(pd.read_csv(file_name))\n",
        "\n",
        "\n",
        "def change_data(feats_tuple, input_row=None): \n",
        "    if input_row:\n",
        "        data_tuple = np.array(input_row)\n",
        "        data_tuple = np.concatenate((data_tuple, data_tuple+15, data_tuple+30, data_tuple+45))\n",
        "        select_file_data = file_data[data_tuple, :]\n",
        "        feats     = select_file_data[:, feats_tuple]\n",
        "        label     = select_file_data[:, -1]\n",
        "        print('-------------------'+str(input_row)+'--------------------------')\n",
        "    else:\n",
        "        feats     = file_data[:, feats_tuple]\n",
        "        label     = file_data[:, -1]\n",
        "    #feats     = preprocessing.StandardScaler().fit_transform(feats)\n",
        "    x_train,x_test,y_train,y_test = train_test_split(feats, label, test_size=0.3, random_state=2)\n",
        "    clf = SVC(C=1000, coef0=0, kernel='rbf', gamma='auto').fit(x_train, y_train) #调参\n",
        "    # acc_dict[str(feats)] =  str(clf.score(x_train, y_train)) + ', ' + str(clf.score(x_test, y_test))\n",
        "    #print('-------------------'+str(feats_tuple)+'--------------------------')\n",
        "    print('train accuracy:' + str(clf.score(x_train, y_train)))\n",
        "    print(confusion_matrix(y_train, clf.predict(x_train)))\n",
        "    print('test accuracy:' + str(clf.score(x_test, y_test)))\n",
        "    print(confusion_matrix(y_test, clf.predict(x_test)))\n",
        "\n",
        "# feats_tuple     = sample(range(0,10), 3)\n",
        "feats_tuple     = (0,1,4, 7,12)    \n",
        "#for feats in  itertools.combinations(feats_tuple, 4):\n",
        "#    change_feats(feats)\n",
        "data_tuple = range(0,15)    \n",
        "for data_comb in  itertools.combinations(data_tuple, 12):\n",
        "    change_data(feats_tuple,data_comb)\n",
        "\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GZJlsDzAq1sv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# feats_tuple     = sample(range(0,10), 3)\n",
        "feats_tuple     = (0,1,4, 7,12)import numpy as np\n",
        "import pandas as pd\n",
        "import csv,os\n",
        "from sklearn import preprocessing\n",
        "\n",
        "\n",
        "output_file = 'subtraction.csv'\n",
        "if os.path.exists(output_file):\n",
        "    os.remove(output_file)\n",
        "    \n",
        "relax = (np.array(pd.read_csv('relax.csv')))[:, :]\n",
        "stress = (np.array(pd.read_csv('stress.csv')))[:, :]\n",
        "\n",
        "subtraction = stress - relax\n",
        "#subtraction = preprocessing.MinMaxScaler().fit_transform(subtraction)\n",
        "for row in subtraction:\n",
        "    out = open(output_file, 'a', newline='')\n",
        "    csv_write = csv.writer(out,dialect='excel')\n",
        "    csv_write.writerow(row)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WukeNLXqUuG0",
        "colab_type": "code",
        "outputId": "cb723c6b-0dd1-4970-fd3b-67f89ebfc665",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 220
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.feature_selection import SelectKBest, chi2,f_regression\n",
        "from sklearn.neighbors import KNeighborsClassifier \n",
        "import numpy as np\n",
        "from sklearn.svm import SVC\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection  import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import pandas as pd\n",
        "from random import sample\n",
        "\n",
        "file_name = 'new_train_feats_data.csv'\n",
        "file_data = np.array(pd.read_csv(file_name))\n",
        "x_train     = file_data[:, :-1]\n",
        "y_train     = file_data[:, -1].reshape((-1,1))\n",
        "\n",
        "#scaler =  preprocessing.MinMaxScaler()\n",
        "#scaler =  preprocessing.StandardScaler()\n",
        "\n",
        "#x_train = scaler.fit_transform(x_train)\n",
        "feats_tuple = SelectKBest(f_regression, k=15).fit(x_train, y_train).get_support(indices = True)\n",
        "print(feats_tuple)\n",
        "feats_tuple = [0, 3, 4, 5, 6]\n",
        "d\n",
        "x_train = x_train[:, feats_tuple]\n",
        "#x_train = scaler.fit_transform(x_train)\n",
        "\n",
        "#knn = KNeighborsClassifier().fit(x_train, y_train)\n",
        "knn = SVC(C=10, coef0=0, kernel='rbf', gamma='auto').fit(x_train, y_train)\n",
        "file_name = 'test_feats_data.csv'\n",
        "file_data = np.array(pd.read_csv(file_name))\n",
        "#x_test     = (scaler.transform(file_data[:,:-1]))[:, feats_tuple]\n",
        "x_test     = file_data[:,:-1][:, feats_tuple]\n",
        "y_test     = file_data[:, -1].reshape((-1,1))\n",
        "#x_train,x_test,y_train,y_test = train_test_split(x_train, y_train, test_size=0.3, random_state=5)\n",
        "                       \n",
        "y_predict = knn.predict(x_test) \n",
        "score=knn.score(x_test,y_test,sample_weight=None)\n",
        "print('train Accuracy:'+str(knn.score(x_train,y_train,sample_weight=None)))\n",
        "print(confusion_matrix(y_train, knn.predict(x_train)))\n",
        "print('test Accuracy:'+str(score))\n",
        "print(confusion_matrix(y_test, knn.predict(x_test)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 16 17]\n",
            "train Accuracy:0.9732142857142857\n",
            "[[56  0]\n",
            " [ 3 53]]\n",
            "test Accuracy:0.4528301886792453\n",
            "[[ 7 20]\n",
            " [ 9 17]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "DyRW3ZcLbL_K",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import make_scorer, fbeta_score, accuracy_score\n",
        "from sklearn.model_selection import GridSearchCV, KFold\n",
        " \n",
        "%matplotlib inline \n",
        " \n",
        "data = pd.read_csv(\"census.csv\")\n",
        " \n",
        "# 将数据切分成特征和标签\n",
        "income_raw = data['income']\n",
        "features_raw = data.drop('income', axis=1)\n",
        " \n",
        "# 显示部分数据\n",
        "# display(features_raw.head(n=1))\n",
        " \n",
        "# 因为原始数据中的，capital-gain 和 capital-loss的倾斜度非常高，所以要是用对数转换。\n",
        "skewed = ['capital-gain', 'capital-loss']\n",
        "features_raw[skewed] = data[skewed].apply(lambda x: np.log(x + 1))\n",
        " \n",
        "# 归一化数字特征,是为了保证所有的特征均被平等的对待\n",
        "scaler = MinMaxScaler()\n",
        "numerical = ['age', 'education-num', 'capital-gain', 'capital-loss', 'hours-per-week']\n",
        "features_raw[numerical] = scaler.fit_transform(data[numerical])\n",
        "# display(features_raw.head(n=1))\n",
        " \n",
        "# 独热编码，将非数字的形式转化为数字\n",
        "features = pd.get_dummies(features_raw)\n",
        "income = income_raw.replace(['>50K', ['<=50K']], [1, 0])\n",
        " \n",
        "# 切分数据集\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, income, test_size=0.2, random_state=0)\n",
        " \n",
        "# Adaboost\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "clf_Ada = AdaBoostClassifier(random_state=0)\n",
        " \n",
        "# 决策树\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "clf_Tree = DecisionTreeClassifier(random_state=0)\n",
        " \n",
        "# KNN\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "clf_KNN = KNeighborsClassifier()\n",
        " \n",
        "# SVM\n",
        "from sklearn.svm import SVC\n",
        "clf_svm = SVC(random_state=0)\n",
        " \n",
        "# Logistic\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "clf_log = LogisticRegression(random_state=0)\n",
        " \n",
        "# 随机森林\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "clf_forest = RandomForestClassifier(random_state=0)\n",
        " \n",
        "# GBDT\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "clf_gbdt = GradientBoostingClassifier(random_state=0)\n",
        " \n",
        "# GaussianNB\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "clf_NB = GaussianNB()\n",
        " \n",
        "scorer = make_scorer(accuracy_score)\n",
        " \n",
        "# 参数调优\n",
        " \n",
        "kfold = KFold(n_splits=10)\n",
        "# 决策树\n",
        "parameter_tree = {'max_depth': xrange(1, 10)}\n",
        "grid = GridSearchCV(clf_Tree, parameter_tree, scorer, cv=kfold)\n",
        "grid = grid.fit(X_train, y_train)\n",
        " \n",
        "print \"best score: {}\".format(grid.best_score_)\n",
        "display(pd.DataFrame(grid.cv_results_).T)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "68R0dZ1lsxRW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#段落"
      ]
    },
    {
      "metadata": {
        "id": "nSrJPLXFYUFy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from panTompkins import panTompkins\n",
        "from timeDomain import timeDomain\n",
        "from frequencyDomain import frequencyDomain\n",
        "import gc, os, platform, csv\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from poincare import eclipseFittingMethod, hraMethod, correlationCoef\n",
        "\n",
        "output_file = 'new_train_feats_data.csv'\n",
        "\n",
        "def new_get_train_feats():\n",
        "    if os.path.exists(output_file):\n",
        "        os.remove(output_file)\n",
        "\n",
        "    get_all_tester_feats('relax')     \n",
        "    get_all_tester_feats('stress')        \n",
        "\n",
        "\n",
        "def get_all_tester_feats(file_type):\n",
        "    assert file_type =='relax' or file_type == 'stress'\n",
        "\n",
        "    if file_type == 'relax':\n",
        "        input_file = 'video1.csv'\n",
        "    elif file_type == 'stress':\n",
        "        input_file = 'video3.csv'\n",
        "\n",
        "    for num in range(4, 19):\n",
        "        if num != 10:\n",
        "            print('-----------------------'+str(num)+'---------------------------')\n",
        "            abs_path = os.getcwd() + '/Data/NO'  + str(num) + '/' + input_file\n",
        "            if (platform.system() == 'Windows'):\n",
        "                abs_path.replace('/', '\\\\')\n",
        "            all_data = (np.array(pd.read_csv(abs_path)))[:,1]\n",
        "            get_single_tester_feats(all_data[     0: 45000], file_type)\n",
        "            get_single_tester_feats(all_data[ 45000: 90000], file_type)\n",
        "            get_single_tester_feats(all_data[ 90000:135000], file_type)\n",
        "            get_single_tester_feats(all_data[135000:      ], file_type)\n",
        "            del all_data\n",
        "            gc.collect()\n",
        "\n",
        "def get_single_tester_feats(csv_data, file_type):\n",
        "    label = 1 if file_type=='stress' else 0\n",
        "    R_peak_locs = panTompkins(ECG=csv_data, fs=300, plot=0)\n",
        "    RR_interval_series = (np.diff(R_peak_locs)) / 300\n",
        "    timeDomainFeats = timeDomain(RR_interval_series)\n",
        "    freqDomainFeats = frequencyDomain(RR_interval_series)\n",
        "    fittingFeats = eclipseFittingMethod(RR_interval_series)\n",
        "    hraFeats = hraMethod(RR_interval_series)\n",
        "    feats_list = [val for val in timeDomainFeats.values()] + \\\n",
        "        [val for val in freqDomainFeats.values()] + \\\n",
        "        [val for val in fittingFeats.values()] + \\\n",
        "        [val for val in hraFeats.values()] + \\\n",
        "        [correlationCoef(RR_interval_series), label]\n",
        "    \n",
        "    if os.path.exists(output_file):\n",
        "        out = open(output_file, 'a', newline='')\n",
        "        csv_write = csv.writer(out,dialect='excel')\n",
        "        csv_write.writerow(feats_list)\n",
        "    else:    # 不存在文件则写入header+data\n",
        "        t_keys = [key for key in timeDomainFeats.keys()]\n",
        "        f_keys = [key for key in freqDomainFeats.keys()]\n",
        "        feats_header_list = [key for key in timeDomainFeats.keys()] + \\\n",
        "            [key for key in freqDomainFeats.keys()] + \\\n",
        "            [key for key in fittingFeats.keys()] + \\\n",
        "            [key for key in hraFeats.keys()] + \\\n",
        "            ['CorrCoef','label']\n",
        "        out = open(output_file, 'a', newline='')\n",
        "        csv_write = csv.writer(out,dialect='excel')\n",
        "        csv_write.writerow(feats_header_list)\n",
        "        csv_write.writerow(feats_list)\n",
        "        \n",
        "new_get_train_feats()        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_OsCL9tuQbVD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# fig函数"
      ]
    },
    {
      "metadata": {
        "id": "lDWJBAJJso5B",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# 画图所需数据库的数据\n",
        "from panTompkins import panTompkins\n",
        "import gc, os, platform, csv\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "input_file  = ('video1.csv', 'video3.csv')\n",
        "output_file = 'train_fig_data.csv'\n",
        "cnt = 1\n",
        "\n",
        "def get_fig_data(csv_data):\n",
        "    global cnt, output_file\n",
        "    R_peak_locs = panTompkins(ECG=csv_data, fs=300, plot=0)\n",
        "    RR_interval_series = (np.diff(R_peak_locs)) / 300\n",
        "\n",
        "    fig_list = [cnt, str(R_peak_locs), str(RR_interval_series.tolist())]\n",
        "    out = open(output_file, 'a', newline='')\n",
        "    csv_write = csv.writer(out,dialect='excel')\n",
        "    if not os.path.exists(output_file):     # write header\n",
        "        fig_header_list = ['index', 'ECG', 'R_location', 'RR_interval']\n",
        "        csv_write.writerow(fig_header_list)  \n",
        "    csv_write.writerow(fig_list)            # write feats\n",
        "    out.close()\n",
        "    cnt += 1\n",
        "\n",
        "    \n",
        "if __name__ == '__main__':\n",
        "    if os.path.exists(output_file):\n",
        "        os.remove(output_file)\n",
        "        \n",
        "    for videocsv in input_file:\n",
        "        for num in range(4, 19):\n",
        "            print('-----------------------'+str(num)+'---------------------------')\n",
        "            abs_path = os.getcwd() + '/Data/NO'  + str(num) + '/' + videocsv\n",
        "            if (platform.system() == 'Windows'):\n",
        "                abs_path.replace('/', '\\\\')\n",
        "            all_data = (np.array(pd.read_csv(abs_path)))[:,1]\n",
        "            get_fig_data(all_data[     0: 45000])\n",
        "            get_fig_data(all_data[ 45000: 90000])\n",
        "            get_fig_data(all_data[ 90000:135000])\n",
        "            get_fig_data(all_data[135000:      ])\n",
        "            del all_data\n",
        "            gc.collect()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Gj4MVPZDTAEu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# 画图所需数据库的数据\n",
        "from panTompkins import panTompkins\n",
        "import gc, os, platform, csv\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "input_file  = 'video2.csv'\n",
        "output_file = 'test_fig_data.csv'\n",
        "cnt = 1\n",
        "\n",
        "def get_fig_data(csv_data):\n",
        "    global cnt, output_file\n",
        "    R_peak_locs = panTompkins(ECG=csv_data, fs=300, plot=0)\n",
        "    RR_interval_series = (np.diff(R_peak_locs)) / 300\n",
        "\n",
        "    fig_list = [cnt, str(R_peak_locs), str(RR_interval_series.tolist())]\n",
        "    out = open(output_file, 'a', newline='')\n",
        "    csv_write = csv.writer(out,dialect='excel')\n",
        "    if not os.path.exists(output_file):     # write header\n",
        "        fig_header_list = ['index', 'R_location', 'RR_interval']\n",
        "        csv_write.writerow(fig_header_list)  \n",
        "    csv_write.writerow(fig_list)            # write feats\n",
        "    out.close()\n",
        "    cnt += 1\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    if os.path.exists(output_file):\n",
        "        os.remove(output_file)\n",
        "\n",
        "    for num in range(4, 19):\n",
        "        print('-----------------------'+str(num)+'---------------------------')\n",
        "        abs_path = os.getcwd() + '/Data/NO'  + str(num) + '/' + input_file\n",
        "        if (platform.system() == 'Windows'):\n",
        "            abs_path.replace('/', '\\\\')\n",
        "        all_data = (np.array(pd.read_csv(abs_path)))[:,1]\n",
        "        get_fig_data(all_data[     0: 54000])\n",
        "        get_fig_data(all_data[ 54000: 90000])\n",
        "        get_fig_data(all_data[ 90000:126000])\n",
        "        get_fig_data(all_data[126000:      ])\n",
        "        del all_data\n",
        "        gc.collect()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Op81o1Z2_D6R",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    }
  ]
}